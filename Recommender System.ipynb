{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required dependencies\n",
    "# use SQLContext to enable simplified querying of the data\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import json\n",
    "\n",
    "# Load the training dataset by specifying its location\n",
    "trainingRawData = sqlContext.read.json('../Datasets/Amazon/path/to/your/data.json')\n",
    "# Extract only the required columns, user ID (reviewerID), product ID (asin), product rating (overall) required by ALS.\n",
    "# Store the data as rdd\n",
    "trainingData = trainingRawData.select(trainingRawData['reviewerID'],trainingRawData['asin'],trainingRawData['overall']).rdd\n",
    "\n",
    "# Calculate average rating of the dataset\n",
    "overallRatingSum = trainingData.map(lambda l: l[2]).sum()\n",
    "mu = float(overallRatingSum/trainingData.count())\n",
    "\n",
    "# Load the test dataset and extract required fields\n",
    "testRawData = sqlContext.read.json('../Datasets/Amazon/path/to/your/data.json')\n",
    "testData = testRawData.select(testRawData['reviewerID'],testRawData['asin'],testRawData['overall']).rdd\n",
    "\n",
    "# ReviewerID and productID are in string format, ALS requires them to be integers, hence a dictionary needs to be created\n",
    "# to map string ID to unique Integer ID\n",
    "\n",
    "# Get all the data from training and test dataset\n",
    "allData = trainingData.union(testData)\n",
    "\n",
    "#Identify unqiue reviewer and product IDs\n",
    "orgReviewerIDs = allData.map(lambda l:l[0]).distinct()\n",
    "orgProductIDs = allData.map(lambda l:l[1]).distinct()\n",
    "\n",
    "# Create a dicitionary of mapping after associating each entry with a unique integer\n",
    "ReviewerIDsMapping = dict(orgReviewerIDs.zipWithUniqueId().collect())\n",
    "ProductIDsMapping = dict(orgProductIDs.zipWithUniqueId().collect())\n",
    "\n",
    "# Create RDD with the newly mapped data. training_RDD now contains integer value for reveiwer ID and product ID and original rating value\n",
    "training_RDD = trainingData.map(lambda l:(ReviewerIDsMapping[l[0]],ProductIDsMapping[l[1]], l[2]))\n",
    "\n",
    "# Extract validation data from training data randomly and cache it for faster processing\n",
    "validation_RDD = training_RDD.sample(False, 0.2, seed = 23).cache()\n",
    "test_RDD = testData.map(lambda l:(ReviewerIDsMapping[l[0]],ProductIDsMapping[l[1]], l[2])).cache()\n",
    "\n",
    "# Find user bias by calculating the average rating given by a user to all products\n",
    "userRatingData = training_RDD.map(lambda l: (l[0],[l[2]])).reduceByKey(lambda x,y : x+y)\n",
    "userRatingData = userRatingData.map(lambda l: (l[0], sum(l[1])/len(l[1])))\n",
    "bu = dict(userRatingData.collect())\n",
    "\n",
    "# Find product bias by calculating the average rating given to a product by all users\n",
    "productRatingData = training_RDD.map(lambda l: (l[1],[l[2]])).reduceByKey(lambda x,y : x+y)\n",
    "productRatingData = productRatingData.map(lambda l: (l[0], sum(l[1])/len(l[1])))\n",
    "bi = dict(productRatingData.collect())\n",
    "\n",
    "# Remove the bias from the original rating\n",
    "training_RDD = training_RDD.map(lambda l: (l[0], l[1], l[2]-mu -(bu[l[0]]-mu)-(bi[l[1]]-mu))).cache()\n",
    "\n",
    "validation_RDD_for_predict = validation_RDD.map(lambda row: (row[0],row[1]))\n",
    "test_RDD_for_predict = test_RDD.map(lambda row: (row[0],row[1]))\n",
    "\n",
    "# Run ALS on unbiased data\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5L\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [10, 20]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "# training phase\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_RDD_for_predict).map(lambda r: ((r[0], r[1]), r[2]+mu+(bu[r[0]]-mu)+(bi[r[1]]-mu)))\n",
    "    \n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print 'The best model was trained with rank %s' % best_rank\n",
    "\n",
    "# testing phase\n",
    "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_RDD_for_predict).map(lambda r: ((r[0], r[1]), r[2]+mu+(bu[r[0]]-mu)+(bi[r[1]]-mu)))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print 'For testing data the RMSE is %s' % (error)\n",
    "\n",
    "rates_and_preds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
